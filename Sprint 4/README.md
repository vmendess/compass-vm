## Estrutura de Pastas
A pasta **Sprint 4** est√° organizada da seguinte forma:

```
Sprint 4/
‚îú‚îÄ Certificados/  # Certificados conquistados durante a sprint, obtivemos certificados nessa sprint
‚îú‚îÄ Desafio/       # Implementa√ß√µes e an√°lises relacionadas ao desafio da sprint
‚îú‚îÄ Evidencias/    # Imagens que comprovam a execu√ß√£o das atividades do desafio
‚îú‚îÄ Exercicios/    # Resolu√ß√µes dos exerc√≠cios pr√°ticos
‚îî‚îÄ README.md      # Documenta√ß√£o da Sprint 4
```

---

## Certificados

Os certificados obtidos durante a Sprint 4 est√£o dispon√≠veis na pasta [Certificados](Certificados/). Obtivemos certificados da AWS nesta sprint.

---
### Desafio

A pasta **[Desafio](Desafio/)** cont√©m os artefatos necess√°rios para as **Etapas 1 e 2** da Sprint 4. Aqui est√° uma vis√£o geral:

- **Etapa 1: Upload e Gerenciamento de Arquivos no S3**  
  Esta etapa envolve a cria√ß√£o de um bucket no **AWS S3**, o upload de um arquivo CSV e a listagem dos arquivos no bucket. Todo o c√≥digo est√° implementado no notebook `etapa1.ipynb`.

- **Etapa 2: Processamento e An√°lise de Dados**  
  Esta etapa √© respons√°vel por carregar os dados do **S3**, limpar e padronizar os valores, realizar an√°lises e salvar os resultados. As implementa√ß√µes est√£o no notebook `etapa2.ipynb`.


---

## Evid√™ncias

As evid√™ncias para valida√ß√£o das an√°lises e execu√ß√µes realizadas est√£o armazenadas na pasta [Evidencias](Evidencias/). Esta pasta cont√©m capturas de tela e outros registros que comprovam a execu√ß√£o das atividades da sprint.

Exemplo de evid√™ncia:

![upload](./Evidencias/upload_csv.png)

---

## Exerc√≠cios - Resumo dos Laborat√≥rios

Os exerc√≠cios foram organizados seguindo a estrutura na Udemy.

## üîπ Laborat√≥rio 1: Amazon S3 - Hospedagem de Site Est√°tico

- Criado um bucket no **Amazon S3** com configura√ß√µes padr√£o na regi√£o **US East (N. Virginia)**.
- Habilitada a hospedagem est√°tica com `index.html` como p√°gina inicial e `404.html` para erros.
- Configuradas permiss√µes p√∫blicas, incluindo pol√≠tica de leitura para acesso externo.
- Feito upload do arquivo `index.html` e um arquivo CSV na pasta `dados`.
- Testado o endpoint gerado, verificando que o site carregou corretamente.

---

## üîπ Laborat√≥rio 2: AWS Athena - Consulta de Dados

- Configurado o Athena para armazenar os resultados na pasta `queries` do bucket S3.
- Criado um banco de dados chamado `meubanco` e uma tabela com base no arquivo `nomes.csv`.
- Executadas consultas, incluindo a listagem dos 3 nomes mais usados por d√©cada desde 1950.
- Resultados das consultas foram salvos no S3.

---

## üîπ Laborat√≥rio 3: AWS Lambda - Processamento com Pandas

- Criada uma fun√ß√£o **Lambda** com runtime **Python 3.9** para processar arquivos no S3.
- Desenvolvido c√≥digo que utiliza **Pandas** para contar linhas de um arquivo CSV e retorna o resultado em JSON.
- Criada uma **layer** contendo **Pandas** e depend√™ncias usando Docker.
- Testada a fun√ß√£o com eventos simulados, garantindo o processamento correto.

---


