{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd1dfc0",
   "metadata": {},
   "source": [
    "\n",
    "# Processamento de Dados com AWS S3 e Pandas\n",
    "\n",
    "Este notebook contém as etapas necessárias para:\n",
    "1. Carregar um arquivo CSV armazenado no **Amazon S3** diretamente no **Pandas**.\n",
    "2. Limpar e processar os dados, padronizando valores e removendo inconsistências.\n",
    "3. Realizar análises estatísticas sobre os dados processados.\n",
    "4. Salvar os resultados das análises em um arquivo `.txt`.\n",
    "\n",
    "## Pré-requisitos\n",
    "\n",
    "### 1. Criar um ambiente virtual\n",
    "1. Criar um ambiente virtual para gerenciar as dependências:\n",
    "    ```bash\n",
    "    python -m venv venv\n",
    "    ```\n",
    "2. Ativar o ambiente virtual:\n",
    "    - **Windows:**  \n",
    "      ```bash\n",
    "      venv\\Scripts\\activate\n",
    "      ```\n",
    "    - **macOS/Linux:**  \n",
    "      ```bash\n",
    "      source venv/bin/activate\n",
    "      ```\n",
    "3. Instalar as bibliotecas necessárias:\n",
    "    ```bash\n",
    "    pip install boto3 pandas python-dotenv\n",
    "    ```\n",
    "\n",
    "### 2. Configurar o arquivo `.env`\n",
    "Criar um arquivo `.env` na mesma pasta do notebook contendo as credenciais da AWS:\n",
    "```\n",
    "AWS_ACCESS_KEY_ID=your_aws_access_key_id\n",
    "AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key\n",
    "AWS_SESSION_TOKEN=your_aws_session_token\n",
    "REGION_NAME=us-east-1\n",
    "```\n",
    "\n",
    "### 3. Selecionar o Kernel no Jupyter Notebook\n",
    "No Jupyter, vá até **Kernel** > **Change Kernel** e selecione **Python (venv)**.\n",
    "\n",
    "Cada etapa está separada em células para facilitar a execução.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befd8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from io import StringIO\n",
    "import unicodedata\n",
    "# Carregar variáveis de ambiente do .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configurações do bucket\n",
    "bucket_name = 'meu-bucket-sprint-4'\n",
    "file_key = 'consumo-energetico-por-campus.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a54d2",
   "metadata": {},
   "source": [
    "\n",
    "## Etapa 1: Carregar Dados do S3\n",
    "Esta função lê o arquivo CSV diretamente do S3 e retorna um DataFrame do Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcde86fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função para carregar dados diretamente do S3 usando boto3\n",
    "def carregar_dados_s3(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Carrega os dados diretamente do S3 usando boto3, sem baixar o arquivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "            aws_session_token=os.getenv('AWS_SESSION_TOKEN')\n",
    "        )\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "        data = response['Body'].read().decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(data), delimiter=';')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados do S3: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec7900",
   "metadata": {},
   "source": [
    "\n",
    "## Etapa 2: Limpeza dos Dados\n",
    "Limpa e converte os dados do DataFrame.\n",
    "\n",
    "Para as colunas numéricas (valor_fatura, quantidade_conceito_faturado, perda_transformacao):\n",
    "- Converte os valores para string.\n",
    "- Substitui vírgulas por pontos (para separador decimal).\n",
    "- Remove caracteres que não sejam dígitos ou ponto.\n",
    "- Converte para float, transformando entradas inválidas em NaN.\n",
    "(Manipulação: Função de conversão)\n",
    "\n",
    "Para as colunas de texto (como localidade e tipo_conceito_faturado):\n",
    "- Converte os valores para string.\n",
    "- Remove acentuação e caracteres especiais usando normalização Unicode.\n",
    "- Converte para maiúsculas para padronização.\n",
    "(Manipulação: Função de string)\n",
    "\n",
    "Ao final, remove linhas duplicadas, mantendo apenas registros distintos.\n",
    "(Manipulação: Remoção de duplicatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a7c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_dados(df):\n",
    "    # Limpeza de colunas numéricas\n",
    "    colunas_numericas = ['valor_fatura', 'quantidade_conceito_faturado', 'perda_transformacao']\n",
    "    for coluna in colunas_numericas:\n",
    "        df[coluna] = (\n",
    "            df[coluna]\n",
    "            .astype(str)\n",
    "            .str.replace(',', '.', regex=False)  # Troca vírgula por ponto\n",
    "            .str.replace(r'[^0-9\\.]', '', regex=True)  # Remove caracteres não numéricos (exceto ponto)\n",
    "        )\n",
    "        # Converte para float; valores inválidos serão transformados em NaN\n",
    "        df[coluna] = pd.to_numeric(df[coluna], errors='coerce')\n",
    "    \n",
    "    # Limpeza de colunas de texto\n",
    "    colunas_texto = []\n",
    "    if 'localidade' in df.columns:\n",
    "        colunas_texto.append('localidade')\n",
    "    if 'tipo_conceito_faturado' in df.columns:\n",
    "        colunas_texto.append('tipo_conceito_faturado')\n",
    "    \n",
    "    for coluna in colunas_texto:\n",
    "        df[coluna] = df[coluna].astype(str).apply(\n",
    "            lambda x: unicodedata.normalize('NFKD', x)   # Decompõe caracteres acentuados\n",
    "                      .encode('ASCII', 'ignore')          # Remove acentos e caracteres especiais\n",
    "                      .decode('utf-8')                    # Decodifica para string\n",
    "                      .upper()                           # Converte para maiúsculas\n",
    "        )\n",
    "    \n",
    "    # Remove linhas duplicadas (100% iguais)\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37e483",
   "metadata": {},
   "source": [
    "\n",
    "## Etapa 3: Análises Estatísticas\n",
    "Aqui são realizadas três análises:\n",
    "1. **Top 10 locais com maior gasto total.**\n",
    "2. **Top 10 locais mais baratos por custo médio.**\n",
    "3. **Ranking de anos por média de valor faturado.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c4d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def realizar_analises(df):\n",
    "    resultados = []\n",
    "    \n",
    "    # ---- Análise 1: Top 10 locais por gasto total (valor_fatura) ----\n",
    "    # Remove registros com localidade nula, vazia ou igual a \"NAN\" (após converter para uppercase).\n",
    "    df_clean = df.dropna(subset=['localidade']).copy()\n",
    "    df_clean = df_clean[df_clean['localidade'].astype(str).str.strip().str.upper() != \"NAN\"]\n",
    "    \n",
    "    # Filtra registros com valor_fatura > 1000 e quantidade_conceito_faturado > 50.\n",
    "    df_a1 = df_clean[(df_clean['valor_fatura'] > 1000) & (df_clean['quantidade_conceito_faturado'] > 50)]\n",
    "    # Agrupa por localidade e soma o valor_fatura.\n",
    "    gasto_por_local = df_a1.groupby('localidade')['valor_fatura'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    resultados.append(\"1. Top 10 locais por gasto total (valor_fatura):\\n\")\n",
    "    for local, gasto in gasto_por_local.head(10).items():\n",
    "        resultados.append(f\"   - {local}: R$ {gasto:,.2f}\\n\")\n",
    "    \n",
    "    # ---- Análise 2: Top 10 locais mais baratos por custo médio de consumo ----\n",
    "    # Seleciona registros com quantidade_conceito_faturado > 0 e localidade válida.\n",
    "    df_a2 = df_clean[df_clean['quantidade_conceito_faturado'] > 0].copy()\n",
    "    # Converte valor_fatura e quantidade_conceito_faturado para float.\n",
    "    df_a2['valor_fatura'] = df_a2['valor_fatura'].astype(float)\n",
    "    df_a2['quantidade_conceito_faturado'] = df_a2['quantidade_conceito_faturado'].astype(float)\n",
    "    # Agrupa por localidade: soma valor_fatura e quantidade.\n",
    "    grupo = df_a2.groupby('localidade').agg({\n",
    "        'valor_fatura': 'sum',\n",
    "        'quantidade_conceito_faturado': 'sum'\n",
    "    })\n",
    "    # Calcula o custo médio de consumo para cada local.\n",
    "    grupo['custo_medio'] = grupo['valor_fatura'] / grupo['quantidade_conceito_faturado']\n",
    "    overall_avg = grupo['custo_medio'].mean()\n",
    "    # Classifica: \"CUSTO BAIXO\" se custo_medio < média geral; senão \"CUSTO ALTO\".\n",
    "    grupo['classificacao'] = grupo['custo_medio'].apply(lambda x: \"CUSTO BAIXO\" if x < overall_avg else \"CUSTO ALTO\")\n",
    "    # Ordena do menor para o maior custo médio.\n",
    "    grupo = grupo.sort_values('custo_medio', ascending=True)\n",
    "    \n",
    "    resultados.append(\"\\n2. Top 10 locais mais baratos por custo médio de consumo (valor_fatura / quantidade):\\n\")\n",
    "    for local, row in grupo.head(10).iterrows():\n",
    "        resultados.append(f\"   - {local}: R$ {row['custo_medio']:,.2f} ({row['classificacao']})\\n\")\n",
    "    \n",
    "    # ---- Análise 3: Ranking de anos por média de valor_fatura ----\n",
    "    # Remove registros com ano nulo ou vazio.\n",
    "    df_a3 = df.dropna(subset=['ano']).copy()\n",
    "    df_a3 = df_a3[df_a3['ano'].astype(str).str.strip() != \"\"]\n",
    "    # Agrupa por ano e calcula a média de valor_fatura.\n",
    "    media_por_ano = df_a3.groupby('ano')['valor_fatura'].mean().sort_values(ascending=False)\n",
    "    \n",
    "    resultados.append(\"\\n3. Ranking de anos por média de valor_fatura:\\n\")\n",
    "    for ano, media in media_por_ano.head(10).items():\n",
    "        resultados.append(f\"   - {ano}: R$ {media:,.2f}\\n\")\n",
    "    \n",
    "    return resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a478ff",
   "metadata": {},
   "source": [
    "\n",
    "## Etapa 4: Salvar Resultados\n",
    "Os resultados são salvos em um arquivo `.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fef15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para salvar resultados em um arquivo TXT\n",
    "def salvar_resultados(resultados, output_file):\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=== Resultados das Análises ===\\n\\n\")\n",
    "            f.writelines(resultados)\n",
    "        print(f\"Resultados salvos em {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar resultados: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15bda74",
   "metadata": {},
   "source": [
    "\n",
    "## Etapa 5: Execução do Processamento\n",
    "Esta etapa **automatiza o processamento dos dados** extraídos do **Amazon S3**, garantindo que todas as transformações sejam aplicadas corretamente. O fluxo é projetado para rodar **de ponta a ponta**, sem necessidade de intervenção manual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a4d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processamento da Etapa 2...\n",
      "Lendo dados diretamente do bucket 'meu-bucket-sprint-4'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vini\\AppData\\Local\\Temp\\ipykernel_64556\\2044927699.py:15: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(StringIO(data), delimiter=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n",
      "Limpando os dados...\n",
      "Realizando análises...\n",
      "Resultados salvos em resultados_analises_etapa2.txt\n",
      "Processamento da Etapa 2 concluído.\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando o processamento da Etapa 2...\")\n",
    "\n",
    "try:\n",
    "    # Carregar dados do S3\n",
    "    print(f\"Lendo dados diretamente do bucket '{bucket_name}'...\")\n",
    "    df = carregar_dados_s3(bucket_name, file_key)\n",
    "    print(\"Dados carregados com sucesso!\")\n",
    "\n",
    "    # Limpar e converter os dados\n",
    "    print(\"Limpando os dados...\")\n",
    "    df = limpar_dados(df)\n",
    "\n",
    "    # Realizar análises\n",
    "    print(\"Realizando análises...\")\n",
    "    resultados = realizar_analises(df)\n",
    "\n",
    "    # Salvar os resultados\n",
    "    salvar_resultados(resultados, 'resultados_analises_etapa2.txt')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro no processamento: {e}\")\n",
    "\n",
    "print(\"Processamento da Etapa 2 concluído.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
